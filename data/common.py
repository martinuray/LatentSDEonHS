"""Utility functions for data loading and pre-processing."""

import torch
import numpy as np


def get_data_min_max(records):
    data_min, data_max = None, None
    inf = torch.Tensor([float("Inf")])[0]

    for b, rc_ in enumerate(records):
        vals, mask = rc_[2], rc_[3]
        n_features = vals.size(-1)

        batch_min = []
        batch_max = []
        for i in range(n_features):
            non_missing_vals = vals[:,i][mask[:,i] == 1]
            if len(non_missing_vals) == 0:
                batch_min.append(inf)
                batch_max.append(-inf)
            else:
                batch_min.append(torch.min(non_missing_vals))
                batch_max.append(torch.max(non_missing_vals))

        batch_min = torch.stack(batch_min)
        batch_max = torch.stack(batch_max)

        if (data_min is None) and (data_max is None):
            data_min = batch_min
            data_max = batch_max
        else:
            data_min = torch.min(data_min, batch_min)
            data_max = torch.max(data_max, batch_max)

    return data_min, data_max


def nanstd(input, dim=None, keepdim=False):
    # Mask NaN values
    mask = ~torch.isnan(input)
    filtered_tensor = input.masked_select(mask)

    # Count non-NaN elements
    non_nan_count = mask.sum(dim=dim, keepdim=keepdim)

    # Compute the mean along the specified dimension while ignoring NaNs
    nanmean = torch.nanmean(input, dim=dim, keepdim=True)

    # Compute the variance manually
    variance = torch.sum(((input - nanmean) * mask) ** 2, dim=dim,
                         keepdim=keepdim) / (non_nan_count - 1)

    # Return square root of variance
    return torch.sqrt(variance)


def normalize_masked_data(data, mask, att_min, att_max):
    att_max[att_max == 0.] = 1. # we don't want to divide by zero

    o_shape = data.shape
    data_r = data.reshape(-1, data.shape[-1])
    mask_r = mask.reshape(-1, mask.shape[-1]).bool()

    if (att_max != 0.).all():
        data_norm = (data - att_min) / att_max
    else:
        raise Exception("Zero!")
    data_mean = data_r.nanmean(0)
    data_std = nanstd(data_r, dim=0)
    data_std[data_std == 0.] = 1e-5 # we don't want to divide by zero

    #data_norm = (data - data_mean) / data_std

    if torch.isnan(data_norm).any():
        raise Exception("nans!")

    # set masked out elements back to 0
    data_norm[mask == 0] = 0

    data_norm = data_norm.reshape(o_shape)

    return data_norm, att_min, att_max


def variable_time_collate_fn(batch, classify=False, activity=False, data_min=None, data_max=None):
    """
    Expects a batch of time series data in the form of (record_id, tt, vals, mask, labels) where
      - record_id is a patient id
      - tt is a 1-dimensional tensor containing T time values of observations.
      - vals is a (T, D) tensor containing observed values for D variables.
      - mask is a (T, D) tensor containing 1 where values were observed and 0 otherwise.
      - labels is a list of labels for the current patient, if labels are available. Otherwise None.
    Returns:
      combined_tt: The union of all time observations.
      combined_vals: (M, T, D) tensor containing the observed values.
      combined_mask: (M, T, D) tensor containing 1 where values were observed and 0 otherwise.
    """
    D = batch[0][2].shape[1]
    # number of labels
    N = batch[0][-1].shape[1] if activity else 1
    len_tt = [ex[1].size(0) for ex in batch]
    maxlen = np.max(len_tt)
    enc_combined_tt = torch.zeros([len(batch), maxlen])
    enc_combined_vals = torch.zeros([len(batch), maxlen, D])
    enc_combined_mask = torch.zeros([len(batch), maxlen, D])
    if classify:
        if activity:
            combined_labels = torch.zeros([len(batch), maxlen, N])
        else:
            combined_labels = torch.zeros([len(batch), N])

    for b, (record_id, tt, vals, mask, labels) in enumerate(batch):
        currlen = tt.size(0)
        enc_combined_tt[b, :currlen] = tt
        enc_combined_vals[b, :currlen] = vals
        enc_combined_mask[b, :currlen] = mask
        if classify:
            if activity:
                combined_labels[b, :currlen] = labels
            else:
                combined_labels[b] = labels

    if not activity:
        enc_combined_vals, _, _ = normalize_masked_data(
            enc_combined_vals,
            enc_combined_mask,
            att_min=data_min,
            att_max=data_max)

    if torch.max(enc_combined_tt) != 0.:
        enc_combined_tt = enc_combined_tt / torch.max(enc_combined_tt)

    combined_data = torch.cat((enc_combined_vals, enc_combined_mask, enc_combined_tt.unsqueeze(-1)), 2)
    if classify:
        return combined_data, combined_labels
    else:
        return combined_data